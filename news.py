import streamlit as st
import requests
import feedparser
from bs4 import BeautifulSoup
from textblob import TextBlob
import pandas as pd
from collections import Counter
import re

st.title("ğŸ“° Google News çˆ¬èŸ²ï¼‹æƒ…ç·’åˆ†æ")
keyword = st.text_input("è«‹è¼¸å…¥è¦æœå°‹çš„é—œéµå­—", "Czech election")

if st.button("é–‹å§‹æœå°‹"):
    with st.spinner("æŠ“å–æ–°èä¸­..."):
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
        }
        url = f"https://news.google.com/rss/search?q={keyword.replace(' ', '+')}"
        response = requests.get(url, headers=headers)
        feed = feedparser.parse(response.content)

        st.write(f"æ‰¾åˆ° {len(feed.entries)} ç­† RSS é …ç›®")

        articles = []

        for entry in feed.entries:
            link = entry.link
            title = entry.title

            try:
                article_html = requests.get(link, headers=headers, timeout=5)
                soup = BeautifulSoup(article_html.content, "html.parser")

                # âœ… å˜—è©¦æŠ“å‰ 10 æ®µè½
                paragraphs = soup.find_all("p")
                text = ' '.join(p.get_text() for p in paragraphs[:10]).strip()

                # âœ… è‹¥å¤ªçŸ­ï¼Œç”¨ <article> å‚™æ´
                if len(text) < 100:
                    article_tag = soup.find("article")
                    if article_tag:
                        text = article_tag.get_text().strip()

                # âœ… è‹¥é‚„æ˜¯å¤ªçŸ­ï¼Œç”¨ <div> å‚™æ´
                if len(text) < 100:
                    div_tag = soup.find("div")
                    if div_tag:
                        text = div_tag.get_text().strip()

                # âœ… æœ€å¾Œå‚™æ´ï¼šç”¨æ¨™é¡Œæƒ…ç·’åˆ†æ
                if not text or len(text) < 50:
                    text = title
                    st.info(f"ğŸŸ¡ åƒ…ç”¨æ¨™é¡Œåšæƒ…ç·’åˆ†æï¼š{link}")

                sentiment = TextBlob(text).sentiment.polarity

                articles.append({
                    "Title": title,
                    "Link": link,
                    "Sentiment": sentiment,
                    "Text": text
                })

            except Exception as e:
                st.warning(f"âš ï¸ æŠ“å–éŒ¯èª¤ï¼š{link}\n{e}")
                continue

        if not articles:
            st.error("ğŸš« æ²’æœ‰æˆåŠŸæŠ“åˆ°æœ‰æ•ˆæ–°è")
        else:
            df = pd.DataFrame(articles)
            st.success(f"ğŸ‰ æˆåŠŸæ“·å– {len(df)} ç­†æ–°è")

            st.dataframe(df[["Title", "Sentiment"]])

            st.subheader("ğŸ¯ æƒ…ç·’æœ€æ­£é¢æ–°è")
            best = df.sort_values("Sentiment", ascending=False).iloc[0]
            st.write(best["Title"])
            st.write(best["Link"])

            st.subheader("ğŸš¨ æƒ…ç·’æœ€è² é¢æ–°è")
            worst = df.sort_values("Sentiment", ascending=True).iloc[0]
            st.write(worst["Title"])
            st.write(worst["Link"])

            all_words = []
            for text in df["Text"]:
                words = re.findall(r'\b[a-zA-Z]{5,}\b', text.lower())
                all_words.extend(words)

            common_words = Counter(all_words).most_common(20)
            st.subheader("ğŸ§  è©é »åˆ†æï¼ˆTop 20ï¼‰")
            for word, count in common_words:
                st.write(f"{word}: {count}")

